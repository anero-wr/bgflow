{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac315563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/ninarell/OneDrive/WF_GAN_FOR_GLASSES/B_GEN/bgflow\")\n",
    "import bgflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c17eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from glob import glob\n",
    "from bgflow.utils import (assert_numpy, distance_vectors, distances_from_vectors,\n",
    "                    remove_mean, IndexBatchIterator, LossReporter)\n",
    "\n",
    "from bgflow import (GaussianMCMCSampler, SequentialFlow, BoltzmannGenerator, \n",
    "                    Energy, Sampler, MultiDoubleWellPotential, MeanFreeNormalDistribution, \n",
    "                    CouplingFlow, AffineTransformer, DenseNet, \n",
    "                    SequentialFlow, SwapFlow, SplitFlow, InverseFlow )\n",
    "from bgflow.utils import RbfEncoder, kernelize_with_rbf, compute_gammas, as_numpy, compute_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6872afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float32\n",
    "\n",
    "# a context tensor to send data to the right device and dtype via '.to(ctx)'\n",
    "ctx = torch.zeros([], device=device, dtype=dtype)\n",
    "\n",
    "# a brief check if this module is the main executable (or imported)\n",
    "main = (__name__ == \"__main__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c19dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = glob('/home/ninarell/OneDrive/WF_GAN_FOR_GLASSES/LJ_CRYSTAL/T_0.700_box/dumplin/dump.npt_nose_T1.0_P0.*.lammpstrj')\n",
    "#fnames = fnames + glob('/home/ninarell/OneDrive/WF_GAN_FOR_GLASSES/LJ_CRYSTAL/T_0.700_box/dumplin/dump.npt_nose_T0.700_P0.0000020000.lammpstrj')\n",
    "#fnames = glob('/home/ninarell/OneDrive/WF_GAN_FOR_GLASSES/LJ_CRYSTAL/T_0.700_box/dumplin/dump.npt_nose_T1.0_P0.0000000000.lammpstrj')\n",
    "arrays = [np.loadtxt(f, skiprows=9)[:,2:5] for f in fnames]\n",
    "coordinates = np.array(arrays)\n",
    "temperature = float(1.00)\n",
    "side=3.\n",
    "dim = len(coordinates)\n",
    "n_particles = len(coordinates[0])\n",
    "spacial_dim = 3\n",
    "dim_ics = n_particles * spacial_dim\n",
    "coordinates=coordinates.reshape(len(arrays), dim_ics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f0c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  bgflow.distribution.energy import LennardJonesPotentialPPP\n",
    "rm = 2**(1./6.)\n",
    "target = LennardJonesPotentialPPP(dim = dim_ics, n_particles = n_particles, side = side, oscillator = False, rm=rm, two_event_dims=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff55f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_energy(coordinates, target):\n",
    "    xs = torch.Tensor(coordinates)\n",
    "    #xs = xs.view(-1,10,3)\n",
    "    energy = target.energy(xs).detach().numpy()\n",
    "    x=np.arange(1,len(energy)+1)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, energy)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    counts, bins = np.histogram(energy, density=True)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.stairs(counts, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df205764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_energy(coordinates, target, step):\n",
    "    xs = torch.Tensor(coordinates)\n",
    "    #xs = xs.view(-1,10,3)\n",
    "    energy = target.energy(xs).detach().numpy()\n",
    "    x=np.arange(1,len(energy)+1)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, energy)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    counts, bins = np.histogram(energy, density=True)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.stairs(counts, bins)\n",
    "    \n",
    "    filename = '/home/ninarell/Desktop/FIG_ENERGY/fig_'+str(step)+\".png\"\n",
    "    fig.savefig(filename, dpi=fig.dpi, format='png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc8188",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_energy(coordinates, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c10909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bgflow import NormalDistribution, TruncatedNormalDistribution\n",
    "\n",
    "mean = torch.Tensor(coordinates[10]).view(dim_ics)\n",
    "cov = torch.zeros(dim_ics, dim_ics)\n",
    "cov = cov.fill_diagonal_(0.001)\n",
    "\n",
    "prior = NormalDistribution(dim_ics, mean=mean, cov=cov, side=side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c80f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prior = prior._sample_with_temperature(100, temperature=temperature)\n",
    "plot_energy(data_prior, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63608d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint(x):\n",
    "    return torch.where((abs(x) > side).clone().detach(), x - 2 * side * torch.sign(x) * torch.floor(abs(x)/side), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482beb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = torch.Tensor(coordinates[10]) #zeros(10,3)\n",
    "\n",
    "mcsampler = GaussianMCMCSampler(target, init_state=init_state, temperature=temperature, box_constraint=constraint, noise_std=0.15)\n",
    "data = mcsampler.sample(10000)\n",
    "data = data.view(-1,dim_ics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_energy(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The equivaraint RNVP flow requires and invaraint scaling transformation \n",
    "# and an equivaraint translation transformation. Both wrt to rotations and permutations\n",
    "\n",
    "class InvariantNet(torch.nn.Module):    \n",
    "    def __init__(self, n_particles, n_dof, dist_net, encoder=None):\n",
    "        super().__init__()\n",
    "        self._dist_net = dist_net\n",
    "        self._encoder = encoder\n",
    "        self._n_particles = n_particles\n",
    "        self._n_dof = n_dof    \n",
    "    def forward(self, x):\n",
    "        n_batch = x.shape[0]\n",
    "        n_dim = self._n_particles * self._n_dof\n",
    "        assert x.shape[-1] == n_dim\n",
    "        x = x.view(n_batch, self._n_particles, self._n_dof)\n",
    "        r = distance_vectors(x) \n",
    "        d = distances_from_vectors(r)\n",
    "        if self._encoder is not None:\n",
    "            d = self._encoder(d.unsqueeze(-1))\n",
    "        d_shape = d.shape\n",
    "        f = self._dist_net(d.view(-1, d_shape[-1]))\n",
    "        f = f.view(*d_shape[:-1], -1)\n",
    "        f = f.view(n_batch, -1).mean(dim=-1, keepdim=True) * torch.ones(n_batch, n_dim) #.cuda()\n",
    "        return f\n",
    "    \n",
    "class EquivariantNet(torch.nn.Module):          \n",
    "    def __init__(self, n_particles, n_dof, dist_net, encoder=None, remove_mean=True):\n",
    "        super().__init__()\n",
    "        self._dist_net = dist_net\n",
    "        self._encoder = encoder\n",
    "        self._invariant_net = dist_net\n",
    "        self._n_particles = n_particles\n",
    "        self._n_dof = n_dof\n",
    "        self._remove_mean = remove_mean  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        n_batch = x.shape[0]\n",
    "        x = x.view(n_batch, self._n_particles, self._n_dof)\n",
    "        r = distance_vectors(x) \n",
    "        dist = distances_from_vectors(r)\n",
    "        r = r / (dist.unsqueeze(-1) + 1e-3)\n",
    "        if self._encoder is not None:\n",
    "            d = self._encoder(dist.unsqueeze(-1)) \n",
    "        d_shape = d.shape\n",
    "        f = self._dist_net(d.view(-1, d_shape[-1]))\n",
    "        f = f.view(*d_shape[:-1], -1) #+ q\n",
    "        f = (f * r).sum(dim=-2)\n",
    "        if self._remove_mean:\n",
    "            f = remove_mean(f, self._n_particles, self._n_dof)\n",
    "        return f.view(n_batch, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2baeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can define the equivariant RNVP flow\n",
    "\n",
    "\n",
    "\n",
    "n_rbfs = 50\n",
    "n_layers = 4\n",
    "\n",
    "# shared distance embeddings across RNVP blocs \n",
    "kernel_mus = torch.linspace(0., 8., n_rbfs)#.cuda()\n",
    "kernel_gammas = compute_gammas(kernel_mus, gain=0.5)#.cuda()\n",
    "dist_encoder = RbfEncoder(kernel_mus, kernel_gammas.log(), trainable=True)\n",
    "\n",
    "layers=[]\n",
    "\n",
    "layers.append(SplitFlow( dim_ics , dim=1))\n",
    "\n",
    "n_coupling_layers = 16\n",
    "for _ in range(n_coupling_layers):\n",
    "    \n",
    "    # we need to swap dimensions for the mixing\n",
    "    layers.append(SwapFlow())\n",
    "    \n",
    "    # now set up a coupling block\n",
    "    layers.append(CouplingFlow(\n",
    "        AffineTransformer(\n",
    "                EquivariantNet(\n",
    "                    n_particles, spacial_dim, DenseNet([n_rbfs, 200, 200, 1], torch.nn.ReLU()),\n",
    "                    encoder=dist_encoder\n",
    "                ),\n",
    "                InvariantNet(\n",
    "                    n_particles, spacial_dim, DenseNet([n_rbfs, 200, 200, 1], torch.nn.ReLU()),\n",
    "                    encoder=dist_encoder\n",
    "                ),\n",
    "\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "# finally, we have to merge the two channels again into one tensor\n",
    "layers.append(InverseFlow(SplitFlow( dim_ics , dim=1)))\n",
    "    \n",
    "# now define the flow as a sequence of all operations stored in layers\n",
    "flow = SequentialFlow(layers) #.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba89d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# having a flow and a prior, we can now define a Boltzmann Generator\n",
    "\n",
    "\n",
    "bg = BoltzmannGenerator(prior, flow, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddabaf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with convex mixture of NLL and KL loss\n",
    "\n",
    "n_kl_samples = 50\n",
    "n_batch = 50\n",
    "batch_iter = IndexBatchIterator(len(data), n_batch)\n",
    "\n",
    "optim = torch.optim.Adam(bg.parameters(), lr=5e-4)\n",
    "\n",
    "n_epochs = 5\n",
    "n_report_steps = 50\n",
    "\n",
    "# mixing parameter\n",
    "lambdas = torch.linspace(1., 0.0, n_epochs)#.cuda()\n",
    "\n",
    "reporter = LossReporter(\"NLL\", \"KLL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef314589",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, lamb in enumerate(lambdas):\n",
    "    for it, idxs in enumerate(batch_iter):\n",
    "        batch_data = data[idxs] #.cuda()\n",
    "        #print(batch_data.shape)\n",
    "        #print(prior.sample(batch_data.shape[0]).shape)\n",
    "        batch_noise = prior.sample(batch_data.shape[0])#.view(-1, dim)\n",
    "        batch = torch.cat([batch_data, batch_noise], dim=1)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "\n",
    "        # negative log-likelihood of the batch is equal to the energy of the BG\n",
    "\n",
    "        latent_samples, latent_dlogp = flow(batch, inverse=True)\n",
    "        nll = - 0.5 * latent_dlogp.mean() + prior.energy(latent_samples[:, dim_ics:]).mean() + prior.energy(latent_samples[:, :dim_ics]).mean()  \n",
    "        (lamb*nll).backward()\n",
    "        \n",
    "        # kl divergence to the target\n",
    "        latent = torch.cat([prior.sample(n_kl_samples), prior.sample(n_kl_samples)], dim=1)\n",
    "        samples, dlogp = flow(latent)\n",
    "\n",
    "        kll = target.energy(samples[:, :dim_ics]).mean() + prior.energy(samples[:, dim_ics:]).mean() - 0.5 *  dlogp.mean()   \n",
    "\n",
    "        # aggregate weighted gradient\n",
    "        ((1. - lamb) * kll).backward()\n",
    "        \n",
    "        reporter.report(nll, kll)\n",
    "        \n",
    "        optim.step()\n",
    "        \n",
    "        if it % n_report_steps == 0:\n",
    "            print(\"\\repoch: {0}, iter: {1}/{2}, lambda: {3}, NLL: {4:.4}, KLL: {5:.4}\".format(\n",
    "                    epoch,\n",
    "                    it,\n",
    "                    len(batch_iter),\n",
    "                    lamb,\n",
    "                    *reporter.recent(1).ravel()\n",
    "                ), end=\"\")\n",
    "    n_samples=100\n",
    "    latent = torch.cat([prior.sample(n_samples), prior.sample(n_samples)], dim=1)\n",
    "    samples, dlogp = flow(latent)\n",
    "    save_energy(samples[:, :dim_ics], target, str(epoch)+\"_\"+str(lamb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae36026",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporter.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13569fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "\n",
    "latent = torch.cat([prior.sample(n_samples), prior.sample(n_samples)], dim=1)\n",
    "samples, dlogp = flow(latent)\n",
    "\n",
    "distances_x = as_numpy(compute_distances(samples[:, :dim_ics], n_particles, dim_ics // n_particles))\n",
    "distances_v = as_numpy(compute_distances(samples[:, dim_ics:], n_particles, dim_ics // n_particles))\n",
    "\n",
    "log_w = target.energy(samples[:, :dim_ics]) + prior.energy(samples[:, dim_ics:]) - 0.5 * dlogp  - prior.energy(latent[:, dim_ics:]) - prior.energy(latent[:, :dim_ics]) \n",
    "log_w = as_numpy(log_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9636be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_energy(samples[:, :dim_ics], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7dfcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bgflow import distances_from_vectors_ppp\n",
    "distances_x = as_numpy(distances_from_vectors_ppp(samples,  side, n_particles, spacial_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d84a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "#plt.plot(d, e, label=\"Groundtruth\", linewidth=4, alpha = 0.5)\n",
    "#data = remove_mean(data, n_particles, spacial_dim)\n",
    "dists_data = as_numpy(distances_from_vectors_ppp(data,  side, n_particles, spacial_dim))\n",
    "#data_prior = remove_mean(data_prior, n_particles, spacial_dim)\n",
    "dists_data_prior = as_numpy(distances_from_vectors_ppp(data_prior,  side, n_particles, spacial_dim))\n",
    "\n",
    "\n",
    "plt.hist(dists_data.reshape(-1), bins=50, label=\"training samples\", alpha=0.5, density=True, histtype='step', linewidth=4);\n",
    "plt.hist(dists_data_prior.reshape(-1), bins=50, label=\"prior samples\", alpha=0.5, density=True, histtype='step', linewidth=4);\n",
    "plt.hist(distances_x.reshape(-1), bins=50, label=\"bg samples\", alpha=0.7, density=True, histtype='step', linewidth=4);\n",
    "#plt.hist(distances_x.reshape(-1), bins=100, label=\"reweighted bg samples\", alpha=0.7, density=True, histtype='step', linewidth=4, weights=np.exp(log_w));\n",
    "plt.xlim(0,10)\n",
    "plt.legend(fontsize=35)\n",
    "plt.xlabel(\"u(x)\", fontsize=45)  \n",
    "plt.xticks(fontsize=45) \n",
    "plt.yticks(fontsize=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb089a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bg",
   "language": "python",
   "name": "bg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
